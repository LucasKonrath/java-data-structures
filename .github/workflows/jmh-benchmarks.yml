name: JMH Benchmarks

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  schedule:
    # Run benchmarks weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    # Allow manual triggering

permissions:
  contents: read
  issues: write
  pull-requests: write

jobs:
  benchmark:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up JDK 11
      uses: actions/setup-java@v4
      with:
        java-version: '11'
        distribution: 'temurin'

    - name: Cache Maven dependencies
      uses: actions/cache@v3
      with:
        path: ~/.m2
        key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}
        restore-keys: ${{ runner.os }}-m2

    - name: Compile project
      run: mvn clean compile

    - name: Build benchmark JAR
      run: mvn package -DskipTests

    - name: Run JMH Benchmarks
      run: java -jar target/benchmarks.jar -rf json -rff target/jmh-results.json

    - name: Parse and Format Benchmark Results
      run: |
        if [ -f target/jmh-results.json ]; then
          echo "## 🚀 JMH Benchmark Results" > benchmark-results.md
          echo "" >> benchmark-results.md
          echo "Generated on: $(date)" >> benchmark-results.md
          echo "" >> benchmark-results.md
          
          # Create a summary table
          echo "| Benchmark | Array Size | Score (μs/op) | Error (±) | Unit |" >> benchmark-results.md
          echo "|-----------|------------|---------------|-----------|------|" >> benchmark-results.md
          
          # Parse JSON and create table rows
          python3 - << 'EOF'
        import json
        import sys
        
        try:
            with open('target/jmh-results.json', 'r') as f:
                data = json.load(f)
            
            # Sort benchmarks by name for consistent ordering
            sorted_benchmarks = sorted(data, key=lambda x: x['benchmark'])
            
            for result in sorted_benchmarks:
                benchmark = result['benchmark'].split('.')[-1]  # Get method name
                score = f"{result['primaryMetric']['score']:.2f}"
                error = f"{result['primaryMetric']['scoreError']:.2f}"
                unit = result['primaryMetric']['scoreUnit']
                
                # Extract array size from benchmark name
                if 'Small' in benchmark:
                    size = "100"
                elif 'Medium' in benchmark:
                    size = "1,000"  
                elif 'Large' in benchmark:
                    size = "10,000"
                else:
                    size = "N/A"
                
                # Clean up benchmark name
                clean_name = benchmark.replace('Small', '').replace('Medium', '').replace('Large', '')
                
                print(f"| {clean_name} | {size} | {score} | {error} | {unit} |")
                
        except Exception as e:
            print(f"Error parsing results: {e}")
            sys.exit(1)
        EOF
        else
          echo "❌ No benchmark results found!"
          exit 1
        fi >> benchmark-results.md

    - name: Add Performance Analysis
      run: |
        echo "" >> benchmark-results.md
        echo "### 📊 Performance Analysis" >> benchmark-results.md
        echo "" >> benchmark-results.md
        echo "**Algorithm Complexity Analysis:**" >> benchmark-results.md
        echo "- **Bubble Sort**: O(n²) - Simple but inefficient for large datasets" >> benchmark-results.md
        echo "- **Insertion Sort**: O(n²) - Efficient for small datasets and nearly sorted arrays" >> benchmark-results.md
        echo "- **Selection Sort**: O(n²) - Consistent performance regardless of input order" >> benchmark-results.md
        echo "- **Merge Sort**: O(n log n) - Consistent performance, stable sort" >> benchmark-results.md
        echo "- **Shell Sort**: O(n log n) to O(n²) - Gap-based insertion sort variant" >> benchmark-results.md
        echo "- **Quick Sort**: O(n log n) average, O(n²) worst - Fast divide-and-conquer algorithm" >> benchmark-results.md
        echo "- **Tree Sort**: O(n log n) average, O(n²) worst - BST-based sorting with duplicate handling" >> benchmark-results.md
        echo "- **Counting Sort**: O(n + k) - Linear time for limited range integers (k = range)" >> benchmark-results.md
        echo "- **Bucket Sort**: O(n + k) - Linear time for limited range (0-9), uses counting buckets" >> benchmark-results.md
        echo "- **Radix Sort**: O(d × (n + k)) - Linear time, d = digits, k = base (10)" >> benchmark-results.md
        echo "" >> benchmark-results.md
        echo "**Key Observations:**" >> benchmark-results.md
        echo "- Linear-time algorithms (Radix, Bucket, Counting Sort) should be fastest for suitable integer ranges" >> benchmark-results.md
        echo "- Radix Sort excels with multi-digit numbers while maintaining linear complexity" >> benchmark-results.md
        echo "- Merge Sort and Quick Sort should perform best on large datasets due to O(n log n) complexity" >> benchmark-results.md
        echo "- Insertion Sort may outperform others on small, nearly-sorted datasets" >> benchmark-results.md
        echo "- Bubble Sort typically shows the worst performance on random data" >> benchmark-results.md
        echo "- Tree Sort performance depends on BST balance and duplicate frequency" >> benchmark-results.md
        echo "- Non-comparison algorithms (Radix/Counting/Bucket) benefit from restricted input domains" >> benchmark-results.md

    - name: Display Results
      run: cat benchmark-results.md

    - name: Upload Benchmark Results
      uses: actions/upload-artifact@v4
      with:
        name: jmh-benchmark-results
        path: |
          target/jmh-results.json
          benchmark-results.md
        retention-days: 30

    - name: Comment on Pull Request
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          if (fs.existsSync('benchmark-results.md')) {
            const results = fs.readFileSync('benchmark-results.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: results
            });
          }

    - name: Create Performance Report Issue
      if: github.event_name == 'schedule'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          if (fs.existsSync('benchmark-results.md')) {
            const results = fs.readFileSync('benchmark-results.md', 'utf8');
            const date = new Date().toISOString().split('T')[0];
            
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `Weekly Performance Report - ${date}`,
              body: `# Weekly Sorting Algorithm Performance Report\n\n${results}\n\n---\n*This report was automatically generated by the JMH benchmark workflow.*`,
              labels: ['performance', 'automated']
            });
          }
